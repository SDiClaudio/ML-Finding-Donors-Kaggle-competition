{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Introduction**\n\nIn this project I will apply supervised machine learning techniques on the US census data to help a fictitious charity organization(CharityML) identify people most likely to donate to their cause.\nFirstly I will perform some preprocessing transformations in order to  to manipulate the data into a workable format. Next, I will evaluate few algorithms of on the data, and consider which is best suited for the solution. Afterwards, I will optimize the selected model and present it as my solution to CharityML.\n\nThe success of the model will be determined based on the model's AUC or area under the curve associated with ROC curves.","metadata":{}},{"cell_type":"code","source":"# importing the necessary libraries\n\nimport time\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\n\nimport xgboost as xgb\n\nimport optuna\n\nfrom sklearn.metrics import accuracy_score, precision_score,recall_score,confusion_matrix, roc_auc_score","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:08.33675Z","iopub.execute_input":"2021-11-22T10:55:08.337237Z","iopub.status.idle":"2021-11-22T10:55:11.452371Z","shell.execute_reply.started":"2021-11-22T10:55:08.337189Z","shell.execute_reply":"2021-11-22T10:55:11.4515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"START = time.time()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.454344Z","iopub.execute_input":"2021-11-22T10:55:11.45473Z","iopub.status.idle":"2021-11-22T10:55:11.463926Z","shell.execute_reply.started":"2021-11-22T10:55:11.454691Z","shell.execute_reply":"2021-11-22T10:55:11.462733Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df=pd.read_csv('../input/udacity-mlcharity-competition/census.csv')\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.465661Z","iopub.execute_input":"2021-11-22T10:55:11.466309Z","iopub.status.idle":"2021-11-22T10:55:11.664401Z","shell.execute_reply.started":"2021-11-22T10:55:11.46627Z","shell.execute_reply":"2021-11-22T10:55:11.663518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Data Exploration","metadata":{}},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.665841Z","iopub.execute_input":"2021-11-22T10:55:11.666177Z","iopub.status.idle":"2021-11-22T10:55:11.718665Z","shell.execute_reply.started":"2021-11-22T10:55:11.666141Z","shell.execute_reply":"2021-11-22T10:55:11.717745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The training data does not contains missing values. It consist of a mix of categorical and numerical features.","metadata":{}},{"cell_type":"code","source":"# checking the label column value distribution\n\ndf.income.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.722821Z","iopub.execute_input":"2021-11-22T10:55:11.7231Z","iopub.status.idle":"2021-11-22T10:55:11.739549Z","shell.execute_reply.started":"2021-11-22T10:55:11.723071Z","shell.execute_reply":"2021-11-22T10:55:11.738717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The target column has a significant imbalance in favor of the <=50K class, which will compromise the model training especially for the lower rapresented class.\nIn order to overcome this problem I can try the following:\n* Use learners capable of dealing with inbalances (i.e. XGBoost)\n* Use oversampling techniques such us SMOTE to create an omogeneous trainig dataset","metadata":{}},{"cell_type":"code","source":"# Replacing the income col values for labels.\n\ndf.income.replace({'<=50K':0,'>50K':1}, inplace=True)\n\nfeatures = df.drop(columns=['income'])\nlabel = df['income']","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.742906Z","iopub.execute_input":"2021-11-22T10:55:11.743269Z","iopub.status.idle":"2021-11-22T10:55:11.78218Z","shell.execute_reply.started":"2021-11-22T10:55:11.743231Z","shell.execute_reply":"2021-11-22T10:55:11.781511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Numerical features","metadata":{}},{"cell_type":"code","source":"numerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\nfor col in numerical:\n    print('\\n')\n    plt.hist(df[col])\n    plt.title(col)\n    plt.show()\n    print(col + ' skewness: ',df[col].skew())","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:11.783245Z","iopub.execute_input":"2021-11-22T10:55:11.78358Z","iopub.status.idle":"2021-11-22T10:55:12.799616Z","shell.execute_reply.started":"2021-11-22T10:55:11.783542Z","shell.execute_reply":"2021-11-22T10:55:12.798021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"As shown by the plots and by the skewness value, *capital-gain* and *capital-loss* are highly skewed.\nIn this cases, using a logarithmic transformation significantly reduces the range of values caused by outliers,so that the very large and very small values do not negatively affect the performance of a learning algorithm. However, I must transoform the values by a small amount above 0 due to the fact that the logarithm of 0 is undefined.","metadata":{}},{"cell_type":"code","source":"# Log-transformation of the skewed features\n\nskewed = ['capital-gain', 'capital-loss']\n\nfeatures_log = features\nfeatures_log[skewed] = features[skewed].apply(lambda x: np.log(x+1))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:12.801367Z","iopub.execute_input":"2021-11-22T10:55:12.80234Z","iopub.status.idle":"2021-11-22T10:55:12.816865Z","shell.execute_reply.started":"2021-11-22T10:55:12.802297Z","shell.execute_reply":"2021-11-22T10:55:12.815863Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Normalization of the numerical features\nscl = MinMaxScaler()\nfeatures_scaled = features_log\nfeatures_scaled[numerical] = scl.fit_transform(features_log[numerical])","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:12.819563Z","iopub.execute_input":"2021-11-22T10:55:12.819809Z","iopub.status.idle":"2021-11-22T10:55:12.886604Z","shell.execute_reply.started":"2021-11-22T10:55:12.819784Z","shell.execute_reply":"2021-11-22T10:55:12.885847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Categorical features","metadata":{}},{"cell_type":"code","source":"#One-Hot-Encoding of the categorical features\n\nfeatures_final = pd.get_dummies(features_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:12.888281Z","iopub.execute_input":"2021-11-22T10:55:12.888794Z","iopub.status.idle":"2021-11-22T10:55:12.946015Z","shell.execute_reply.started":"2021-11-22T10:55:12.888756Z","shell.execute_reply":"2021-11-22T10:55:12.945293Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features_final","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:12.948276Z","iopub.execute_input":"2021-11-22T10:55:12.948824Z","iopub.status.idle":"2021-11-22T10:55:12.975576Z","shell.execute_reply.started":"2021-11-22T10:55:12.948783Z","shell.execute_reply":"2021-11-22T10:55:12.974746Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Kaggle Test","metadata":{}},{"cell_type":"markdown","source":"Now I take a look at the Kaggle test set","metadata":{}},{"cell_type":"code","source":"kaggle = pd.read_csv('../input/udacity-mlcharity-competition/test_census.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:12.977258Z","iopub.execute_input":"2021-11-22T10:55:12.977641Z","iopub.status.idle":"2021-11-22T10:55:13.151887Z","shell.execute_reply.started":"2021-11-22T10:55:12.977602Z","shell.execute_reply":"2021-11-22T10:55:13.151087Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"kaggle.info()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.153178Z","iopub.execute_input":"2021-11-22T10:55:13.153547Z","iopub.status.idle":"2021-11-22T10:55:13.200997Z","shell.execute_reply.started":"2021-11-22T10:55:13.153507Z","shell.execute_reply":"2021-11-22T10:55:13.200141Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Upon inspection of the kaggle test data, I found an extra field named *Unamed: 0* and also that every other features contains missing values.\nI will now proceed in dropping the extra column and filling the missing values with the training dataset most frequent value for that same column.\n\nAfterwards, I will apply the same trasformations for both numerical and categorical features accordingly utilized in the training data.","metadata":{}},{"cell_type":"code","source":"# dropping the extra column\n\nkaggle.drop(columns=['Unnamed: 0'], inplace=True)\nkaggle","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.202312Z","iopub.execute_input":"2021-11-22T10:55:13.202796Z","iopub.status.idle":"2021-11-22T10:55:13.237194Z","shell.execute_reply.started":"2021-11-22T10:55:13.202756Z","shell.execute_reply":"2021-11-22T10:55:13.236345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in kaggle.columns:\n    kaggle[col].fillna(features[col].mode()[0], inplace=True)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.238487Z","iopub.execute_input":"2021-11-22T10:55:13.238801Z","iopub.status.idle":"2021-11-22T10:55:13.325467Z","shell.execute_reply.started":"2021-11-22T10:55:13.238772Z","shell.execute_reply":"2021-11-22T10:55:13.324616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''for col in kaggle.columns:\n\n    kaggle[col].fillna(kaggle[col].mode()[0], inplace=True)'''","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2021-11-22T10:55:13.326804Z","iopub.execute_input":"2021-11-22T10:55:13.327201Z","iopub.status.idle":"2021-11-22T10:55:13.334205Z","shell.execute_reply.started":"2021-11-22T10:55:13.327161Z","shell.execute_reply":"2021-11-22T10:55:13.333266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying the same transformations used in the training data\n\nskewed = ['capital-gain', 'capital-loss']\nnumerical = ['age', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n\n# Log-transformation of the skewed featurez\nfeaturez_log = kaggle\nfeaturez_log[skewed] = featurez_log[skewed].apply(lambda x: np.log(x+1))\n\n# Applying the scaler to the featurez\nfeaturez_scaled = featurez_log\nfeaturez_scaled[numerical] = scl.fit_transform(featurez_log[numerical])\n\n# OneHotEncoding of the categorical features\nfeaturez_final = pd.get_dummies(featurez_scaled)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.335649Z","iopub.execute_input":"2021-11-22T10:55:13.33617Z","iopub.status.idle":"2021-11-22T10:55:13.454581Z","shell.execute_reply.started":"2021-11-22T10:55:13.336129Z","shell.execute_reply":"2021-11-22T10:55:13.453838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Modeling","metadata":{}},{"cell_type":"code","source":"# Split the 'features' and 'income' data into training and testing sets\nX_train, X_val, y_train, y_val = train_test_split(features_final, \n                                                    label, \n                                                    test_size = 0.2)","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.456025Z","iopub.execute_input":"2021-11-22T10:55:13.456431Z","iopub.status.idle":"2021-11-22T10:55:13.479216Z","shell.execute_reply.started":"2021-11-22T10:55:13.456381Z","shell.execute_reply":"2021-11-22T10:55:13.478483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# calculating the pos_weight value for xgboost\n\npos_weight = round((df.income.value_counts()[0])/(df.income.value_counts()[1]))\npos_weight","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.480813Z","iopub.execute_input":"2021-11-22T10:55:13.481313Z","iopub.status.idle":"2021-11-22T10:55:13.491361Z","shell.execute_reply.started":"2021-11-22T10:55:13.481272Z","shell.execute_reply":"2021-11-22T10:55:13.49029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Applying SMOTE to the training set in order to resolve the target variable values inbalance\n\nsm = SMOTE()\nX_train, y_train = sm.fit_resample(X_train,y_train)\n\ny_train.value_counts()","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:13.49309Z","iopub.execute_input":"2021-11-22T10:55:13.493571Z","iopub.status.idle":"2021-11-22T10:55:20.163442Z","shell.execute_reply.started":"2021-11-22T10:55:13.493533Z","shell.execute_reply":"2021-11-22T10:55:20.16254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"It is now time to run few learners on the training data in order to compare their performances. \nI will test the performance of four learners: AdaBoost, RandomForest, XGBoost and SVM.\nI will then create a voting model with the best three in order to create the predictions for the submission.","metadata":{}},{"cell_type":"code","source":"# Initializing the learners\n\nada = AdaBoostClassifier(base_estimator=DecisionTreeClassifier())\nrf = RandomForestClassifier()\nxgb_boost = xgb.XGBClassifier(eval_metric='logloss', tree_method='gpu_hist')\nsvm = SVC(probability=True)\n\nclf_list = [ada, rf, xgb_boost,svm]\nclf_names = ['AdaBoost', 'Random Forest', 'XGBoost','SVM']","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:20.164769Z","iopub.execute_input":"2021-11-22T10:55:20.165139Z","iopub.status.idle":"2021-11-22T10:55:20.17236Z","shell.execute_reply.started":"2021-11-22T10:55:20.165087Z","shell.execute_reply":"2021-11-22T10:55:20.171511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nresults = {'Algorithm' : 'xxx',\n      'Accuracy':'xxx',\n      'Precision':'xxx',\n      'Recall':'xxx'}\n\nresults = pd.DataFrame(results, index=[0])\n\nfor n, clf in enumerate(clf_list):\n    \n    clf.fit(X_train, y_train)\n    y_preds = clf.predict(X_val)\n    y_pred_probs = clf.predict_proba(X_val)[:,1]\n    \n    rw = {'Algorithm' : clf_names[n],\n          'Accuracy':accuracy_score(y_val, y_preds),\n          'Precision':precision_score(y_val, y_preds),\n          'Recall':recall_score(y_val, y_preds), \n          'ROC':roc_auc_score(y_val,y_pred_probs)}\n    \n    rw = pd.DataFrame(rw, index=[0])\n    results = pd.concat([results,rw])\n    \n    print('\\n\\n')\n    print(clf_names[n] + ' Confusion Matrix')\n    print(confusion_matrix(y_val, y_preds))\n    \n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T10:55:20.173874Z","iopub.execute_input":"2021-11-22T10:55:20.174252Z","iopub.status.idle":"2021-11-22T11:20:14.037224Z","shell.execute_reply.started":"2021-11-22T10:55:20.174213Z","shell.execute_reply":"2021-11-22T11:20:14.036083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"results = results.iloc[1:]\nresults","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:20:14.039027Z","iopub.execute_input":"2021-11-22T11:20:14.03968Z","iopub.status.idle":"2021-11-22T11:20:14.053905Z","shell.execute_reply.started":"2021-11-22T11:20:14.039639Z","shell.execute_reply":"2021-11-22T11:20:14.052907Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Viszualizing  model performaces\n\nfig,(ax1,ax2,ax3,ax4) = plt.subplots(1,4,figsize=(20,8))\n\nax1.bar(x=results['Algorithm'], height=results['Precision'])\nax1.title.set_text('Precision')\nax1.tick_params(axis='x', rotation=45)\nax2.bar(x=results['Algorithm'], height=results['Recall'])\nax2.title.set_text('Recall')\nax2.tick_params(axis='x', rotation=45)\nax3.bar(x=results['Algorithm'], height=results['Accuracy'])\nax3.title.set_text('Accuracy')\nax3.tick_params(axis='x', rotation=45)\nax4.bar(x=results['Algorithm'], height=results['ROC'])\nax4.title.set_text('ROC AUC score')\nax4.tick_params(axis='x', rotation=45);","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:20:14.055924Z","iopub.execute_input":"2021-11-22T11:20:14.056361Z","iopub.status.idle":"2021-11-22T11:20:14.47959Z","shell.execute_reply.started":"2021-11-22T11:20:14.056317Z","shell.execute_reply":"2021-11-22T11:20:14.478731Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"* **XGBoost:** is the best perfoming learner overall\n* **AdaBoost:** has a better performance for the recall than the precision. Good also the ROC value\n* **RandomForest:** is the second best performing learner overall. Similar scores to AdaBoost, just a tiny better\n* **SVM:** the best recall score of the four learners, but the worse precision.Good value for the ROC, however the training takes significantly more time.","metadata":{}},{"cell_type":"markdown","source":"In order to carry out the voting between the best three performers, I create a function which averages the output of the three learners. Taking the average is important as I will use predict_proba() instead of predict(). The former is a more accurate way of predicting when the evaluation metric is the AUC or area under the curve associated with ROC curves.","metadata":{}},{"cell_type":"code","source":"def voting_predictor(clf_1,clf_2, clf_3,cols_names,file_name):\n    \n    y1_pred = clf_1.predict_proba(featurez_final)[:,1].reshape(featurez_final.shape[0],1)\n    y2_pred = clf_2.predict_proba(featurez_final)[:,1].reshape(featurez_final.shape[0],1)\n    y3_pred = clf_3.predict_proba(featurez_final)[:,1].reshape(featurez_final.shape[0],1)\n    \n    y_pred = pd.DataFrame(np.concatenate([y1_pred,y2_pred,y3_pred], axis=1),columns=cols_names)\n    y_pred = y_pred.mean(axis=1, numeric_only=True).reset_index()\n    y_pred.columns = ['id','income']\n    \n    y_pred.to_csv(file_name, index=False)\n  \n\n    \n    y1_pred_test = clf_1.predict_proba(X_val)[:,1].reshape(X_val.shape[0],1)\n    y2_pred_test = clf_2.predict_proba(X_val)[:,1].reshape(X_val.shape[0],1)\n    y3_pred_test = clf_3.predict_proba(X_val)[:,1].reshape(X_val.shape[0],1)\n    \n    y_pred_test = pd.DataFrame(np.concatenate([y1_pred_test,y2_pred_test,y3_pred_test], axis=1),columns=cols_names)\n    y_pred_test = y_pred_test.mean(axis=1, numeric_only=True)\n    \n    print('\\n',roc_auc_score(y_val,y_pred_test))\n    \n    return print('DONE!')","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:20:14.481278Z","iopub.execute_input":"2021-11-22T11:20:14.481645Z","iopub.status.idle":"2021-11-22T11:20:14.494519Z","shell.execute_reply.started":"2021-11-22T11:20:14.481607Z","shell.execute_reply":"2021-11-22T11:20:14.493437Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# XGBoost Optimization\n\nI now carry out the optimization of the learners using Optuna and ROC as scoring metric.","metadata":{}},{"cell_type":"markdown","source":"**Optuna SearchCV**\n\n","metadata":{}},{"cell_type":"code","source":"param_distributions = {\n    'eta': optuna.distributions.UniformDistribution(0.01,0.1),\n    'max_depth': optuna.distributions.IntUniformDistribution(4,10),\n    'gamma': optuna.distributions.UniformDistribution(0,5),\n    'n_estimators': optuna.distributions.IntUniformDistribution(150,1500),\n    'booster': optuna.distributions.CategoricalDistribution(['gbtree', 'dart']),\n    'pos_weight': optuna.distributions.UniformDistribution(0,pos_weight)\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:20:14.495837Z","iopub.execute_input":"2021-11-22T11:20:14.496342Z","iopub.status.idle":"2021-11-22T11:20:14.506619Z","shell.execute_reply.started":"2021-11-22T11:20:14.496303Z","shell.execute_reply":"2021-11-22T11:20:14.505836Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\n\nxgb_opt_search = optuna.integration.OptunaSearchCV(estimator=xgb_boost,\n                                               param_distributions = param_distributions,\n                                               cv=5,\n                                               n_jobs=-1,\n                                               n_trials = 10,\n                                               scoring='roc_auc',\n                                               verbose=10)\n\nxgb_opt_search.fit(X_train, y_train)\ny_preds = xgb_opt_search.predict_proba(X_val)[:,1]\n\nprint('\\n','Best Score',xgb_opt_search.best_score_)\nprint('\\n','Best Params',xgb_opt_search.best_estimator_.get_params())\nprint('\\n',roc_auc_score(y_val,y_preds))\n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T11:20:14.509938Z","iopub.execute_input":"2021-11-22T11:20:14.510294Z","iopub.status.idle":"2021-11-22T12:18:16.7093Z","shell.execute_reply.started":"2021-11-22T11:20:14.510265Z","shell.execute_reply":"2021-11-22T12:18:16.708512Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AdaBoost optimization","metadata":{}},{"cell_type":"markdown","source":"**Optuna SearchCV**","metadata":{}},{"cell_type":"code","source":"param_distributions = {\n    'base_estimator__criterion': optuna.distributions.CategoricalDistribution(['gini','entropy']),\n    'base_estimator__max_features':optuna.distributions.CategoricalDistribution(['auto','sqrt','log2']),\n    'base_estimator__max_depth':optuna.distributions.IntUniformDistribution(4,15),\n    'n_estimators':optuna.distributions.IntUniformDistribution(150,1500),\n    'learning_rate':optuna.distributions.UniformDistribution(0.01,0.1)\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:18:16.710754Z","iopub.execute_input":"2021-11-22T12:18:16.711277Z","iopub.status.idle":"2021-11-22T12:18:16.718358Z","shell.execute_reply.started":"2021-11-22T12:18:16.711242Z","shell.execute_reply":"2021-11-22T12:18:16.717495Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nada_opt_search = optuna.integration.OptunaSearchCV(estimator=ada,\n                                               param_distributions = param_distributions,\n                                               cv=5,\n                                               n_jobs=-1,\n                                               n_trials = 10,\n                                               scoring='roc_auc',\n                                               verbose=10)\n\nada_opt_search.fit(X_train, y_train)\ny_preds = ada_opt_search.predict_proba(X_val)[:,1]\n\nprint('\\n','Best Score',ada_opt_search.best_score_)\nprint('\\n','Best Params',ada_opt_search.best_estimator_.get_params())\nprint('\\n',roc_auc_score(y_val,y_preds))\n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:18:16.720114Z","iopub.execute_input":"2021-11-22T12:18:16.720833Z","iopub.status.idle":"2021-11-22T12:46:57.864771Z","shell.execute_reply.started":"2021-11-22T12:18:16.720793Z","shell.execute_reply":"2021-11-22T12:46:57.863913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Random Forest optimization","metadata":{}},{"cell_type":"markdown","source":"**Optuna SearchCV**","metadata":{}},{"cell_type":"code","source":"param_distributions = {\n    'criterion': optuna.distributions.CategoricalDistribution(['gini','entropy']),\n    'max_features':optuna.distributions.CategoricalDistribution(['auto','sqrt','log2']),\n    'max_depth':optuna.distributions.IntUniformDistribution(4,15),\n    'n_estimators':optuna.distributions.IntUniformDistribution(150,1500)\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:46:57.866088Z","iopub.execute_input":"2021-11-22T12:46:57.866458Z","iopub.status.idle":"2021-11-22T12:46:57.873343Z","shell.execute_reply.started":"2021-11-22T12:46:57.8664Z","shell.execute_reply":"2021-11-22T12:46:57.872479Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"start_time = time.time()\n\nrf_opt_search = optuna.integration.OptunaSearchCV(estimator=rf,\n                                               param_distributions = param_distributions,\n                                               cv=5,\n                                               n_jobs=-1,\n                                               n_trials = 10,\n                                               scoring='roc_auc',\n                                               verbose=10)\n\nrf_opt_search.fit(X_train, y_train)\ny_preds = rf_opt_search.predict_proba(X_val)[:,1]\n\nprint('\\n','Best Score',rf_opt_search.best_score_)\nprint('\\n','Best Params',rf_opt_search.best_estimator_.get_params())\nprint('\\n',roc_auc_score(y_val,y_preds))\n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T12:46:57.874634Z","iopub.execute_input":"2021-11-22T12:46:57.87513Z","iopub.status.idle":"2021-11-22T13:00:15.750312Z","shell.execute_reply.started":"2021-11-22T12:46:57.875091Z","shell.execute_reply":"2021-11-22T13:00:15.749391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Voting\nstart_time = time.time()\n\nvote_clf = VotingClassifier(estimators=['xgb',xgb_opt_search,\n                                        'ada',ada_opt_search,\n                                        'rf', rf_opt_search],\n                            voting='soft')\n\nvote_clf_fitted = vote_clf.fit(X_train,y_train) # features_final & label here?\npreds = vote_clf_fitted.predict_proba(featurez_final)[:,1].reshape(featurez_final.shape[0],1)\n\n\n\n\n\n\n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T13:12:18.257976Z","iopub.execute_input":"2021-11-22T13:12:18.258296Z","iopub.status.idle":"2021-11-22T13:12:18.288763Z","shell.execute_reply.started":"2021-11-22T13:12:18.258264Z","shell.execute_reply":"2021-11-22T13:12:18.287103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# SVM optimization","metadata":{}},{"cell_type":"markdown","source":"param_distributions = {\n    'C':optuna.distributions.UniformDistribution(1,5)\n}\n\nfrom datetime import datetime\n\nnow = datetime.now()\n\ncurrent_time = now.strftime(\"%H:%M:%S\")\nprint(\"Current Time =\", current_time)","metadata":{}},{"cell_type":"markdown","source":"#Time = 13 secs\n#%%time\nvoting_predictor(xgb_opt_search,ada_opt_search,rf_opt_search,cols_names=['xgb','ada', 'rf'],\n                 file_name='./xgb ada rf opt POS_WEIGHT and fillna feat_mode voting_preds.csv')","metadata":{}},{"cell_type":"markdown","source":"start_time = time.time()\n\nsvm_opt_search = optuna.integration.OptunaSearchCV(estimator=svm,\n                                               param_distributions = param_distributions,\n                                               cv=5,\n                                               n_jobs=-1,\n                                               n_trials = 10,\n                                               scoring='roc_auc',\n                                               verbose=10)\n\nsvm_opt_search.fit(X_train_res, y_train_res)\ny_preds = svm_opt_search.predict_proba(X_val)[:,1]\n\nprint('\\n','Best Score',svm_opt_search.best_score_)\nprint('\\n','Best Params',svm_opt_search.best_estimator_.get_params())\nprint('\\n',roc_auc_score(y_val,y_preds))\n\nend_time = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((end_time-start_time))))","metadata":{"execution":{"iopub.status.busy":"2021-11-21T09:48:16.064362Z","iopub.execute_input":"2021-11-21T09:48:16.06498Z","iopub.status.idle":"2021-11-21T13:26:56.873521Z","shell.execute_reply.started":"2021-11-21T09:48:16.064939Z","shell.execute_reply":"2021-11-21T13:26:56.869999Z"}}},{"cell_type":"markdown","source":"# Time = 2min\n%%time\nvoting_predictor(xgb_opt_search,svm_opt_search,rf_opt_search,cols_names=['xgb','svm', 'rf'],\n                 file_name='./xgb svm rf opt SMOTE corrected and fillna feat_mode voting_preds.csv')","metadata":{"execution":{"iopub.status.busy":"2021-11-21T13:26:56.876278Z","iopub.status.idle":"2021-11-21T13:26:56.878567Z"}}},{"cell_type":"code","source":"# Overall time = \n\nEND = time.time()\nprint('\\n',time.strftime(\"%Hh%Mm%Ss\", time.gmtime((END-START))))","metadata":{"execution":{"iopub.status.busy":"2021-11-22T13:00:15.768891Z","iopub.status.idle":"2021-11-22T13:00:15.769668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}